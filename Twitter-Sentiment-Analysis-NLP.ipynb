{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-05-05T15:08:40.443Z"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries required to load, transform, analyze and plot data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(context='paper', style='darkgrid', \n",
    "        rc={'figure.facecolor':'white'}, font_scale=1.2)\n",
    "\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import spacy\n",
    "from spacy.tokenizer import _get_regex_pattern\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T15:08:45.715361Z",
     "start_time": "2021-05-05T15:08:45.665544Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ba20779f1b41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# remove scientific notation and restrictions on df rows/columns display\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'{:,.3f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'display.max_columns'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'max_rows'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'display.max_colwidth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# remove scientific notation and restrictions on df rows/columns display\n",
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('max_rows', 200)\n",
    "pd.set_option('display.max_colwidth', 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T15:08:45.716847Z",
     "start_time": "2021-05-05T15:08:45.066Z"
    }
   },
   "outputs": [],
   "source": [
    "# load primary source file to df, renaming columns, dropping non-ASCII\n",
    "col_names = ['tweet_text', 'directed_at', 'emotion_label']\n",
    "tweets = pd.read_csv('data/judge-1377884607_tweet_product_company.csv', encoding= 'unicode_escape', names=col_names, header=0)\n",
    "tweets.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T14:20:50.992711Z",
     "start_time": "2021-05-05T14:20:50.970720Z"
    }
   },
   "outputs": [],
   "source": [
    "# review data types and null counts\n",
    "tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T14:20:51.885519Z",
     "start_time": "2021-05-05T14:20:51.870985Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop nan tweets from dataframe\n",
    "tweets.dropna(subset = ['tweet_text'], inplace=True)\n",
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T14:20:53.812142Z",
     "start_time": "2021-05-05T14:20:53.802493Z"
    }
   },
   "outputs": [],
   "source": [
    "# check value counts by column\n",
    "print(tweets['directed_at'].value_counts(normalize=True, dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T14:20:54.650128Z",
     "start_time": "2021-05-05T14:20:54.636595Z"
    }
   },
   "outputs": [],
   "source": [
    "# create brand feature\n",
    "tweets['directed_at'].fillna('None', inplace=True)\n",
    "brand_map = {'iPad': 'Apple', 'Apple': 'Apple', 'iPad or iPhone App': 'Apple', \n",
    "             'Google': 'Google', 'iPhone': 'Apple', \n",
    "             'Other Google product or service': 'Google',\n",
    "            'Android App': 'Google', 'Android': 'Google',\n",
    "             'Other Apple product or service': 'Apple',\n",
    "             'None': 'None'\n",
    "            }\n",
    "tweets['brand'] = tweets.directed_at.map(brand_map, na_action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T14:20:56.037033Z",
     "start_time": "2021-05-05T14:20:56.018377Z"
    }
   },
   "outputs": [],
   "source": [
    "# clean emotion labels\n",
    "tweets['emotion_label'].replace({'No emotion toward brand or product': 'Neutral',\n",
    "                                 'Positive emotion': 'Positive', \n",
    "                                 'Negative emotion': 'Negative', \n",
    "                                 'I can\\'t tell': 'Unknown'}, inplace=True)\n",
    "\n",
    "# check value counts by column\n",
    "print(tweets['emotion_label'].value_counts(normalize=True, dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T14:20:57.621726Z",
     "start_time": "2021-05-05T14:20:57.601786Z"
    }
   },
   "outputs": [],
   "source": [
    "# check value counts by column\n",
    "tweets.groupby(by=['brand', 'emotion_label'])['tweet_text'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Text Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T14:21:00.721979Z",
     "start_time": "2021-05-05T14:21:00.713879Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get all the stop words and punctuation in the English language\n",
    "punctuation = list(string.punctuation)\n",
    "punctuation.remove('#') # keep # for hashtags\n",
    "\n",
    "# sample stopwods and punctuations\n",
    "print(f'Punctuation Count: {len(punctuation)} Sample 5: {punctuation[0:5]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T14:21:02.460849Z",
     "start_time": "2021-05-05T14:21:02.452690Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(x):\n",
    "    \"\"\"\n",
    "    Helper function to remove punctuation from a string x: any string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        x = re.sub('@[A-Za-z0-9]+', '', x) # remove @mention users\n",
    "        x = re.sub(r'http\\S+', '', x) # remove URL references\n",
    "        x = re.sub(r'\\b[0-9]+\\b', '', x) # remove stand-alone numbers  \n",
    "        x = ''.join(ch for ch in x if ch not in punctuation) # remove punc\n",
    "    except:\n",
    "        pass\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T14:21:03.311633Z",
     "start_time": "2021-05-05T14:21:03.307363Z"
    }
   },
   "outputs": [],
   "source": [
    "# function to clean text\n",
    "def  clean_text(df, text_field, new_text_field):\n",
    "    df[new_text_field] = df[text_field].str.lower()\n",
    "    df[new_text_field] = df[new_text_field].apply(remove_punctuation) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T14:21:04.495455Z",
     "start_time": "2021-05-05T14:21:04.019175Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets_clean = clean_text(tweets, 'tweet_text', 'tweet_text_clean')\n",
    "tweets_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T14:21:07.934155Z",
     "start_time": "2021-05-05T14:21:07.380424Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# get default pattern for tokens that don't get split\n",
    "re_token_match = _get_regex_pattern(nlp.Defaults.token_match)\n",
    "# add your patterns (here: hashtags and in-word hyphens)\n",
    "re_token_match = f\"({re_token_match}|#\\w+|\\w+-\\w+)\"\n",
    "\n",
    "# overwrite token_match function of the tokenizer\n",
    "nlp.tokenizer.token_match = re.compile(re_token_match).match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T14:21:55.385590Z",
     "start_time": "2021-05-05T14:21:08.715570Z"
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "stops_sp = nlp.Defaults.stop_words\n",
    "print(f'spaCy Stopword Count: {len(stops_sp)}')\n",
    "\n",
    "def clean_token(doc):\n",
    "    return [token.lemma_ for token in doc if not token.is_stop \n",
    "            and not token.is_punct and not token.is_digit \n",
    "            and not token.is_space]\n",
    "\n",
    "tweets['tokens_sp'] = [clean_token(nlp(row)) for row in tweets.tweet_text_clean.apply(str)]\n",
    "\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T20:08:15.886715Z",
     "start_time": "2021-05-04T20:08:15.832449Z"
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "word_dict = {}\n",
    "\n",
    "# Loop through all the tags\n",
    "for i, row in tweets['tokens_sp'].iteritems():\n",
    "    for word in row:\n",
    "        if word not in word_dict:\n",
    "            word_dict[word] = 1\n",
    "        else:\n",
    "            word_dict[word] +=1\n",
    "\n",
    "word_counts = sorted(word_dict.items(), key=lambda kv: kv[1], reverse=True)\n",
    "print(f'Total words: {len(word_counts)}')\n",
    "word_counts[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T14:59:37.281960Z",
     "start_time": "2021-05-05T14:59:37.264466Z"
    }
   },
   "outputs": [],
   "source": [
    "# filter tweets for identifiable emotions only (drop unknown)\n",
    "sentiments = ['Positive', 'Negative', 'Neutral']\n",
    "tweets_f = tweets[tweets['emotion_label'].isin(sentiments)]\n",
    "\n",
    "# create X and y (one-hot encoded for 3 classes)\n",
    "X = tweets_f['tokens_sp']\n",
    "y = pd.get_dummies(tweets_f['emotion_label'])\n",
    "print(X.iloc[:3], y.iloc[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T14:52:52.798738Z",
     "start_time": "2021-05-05T14:52:52.719077Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split into training and test sets\n",
    "SEED = 19\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=SEED)\n",
    "print(f'X_train: {X_train.shape} X_test: {X_test.shape} ' \n",
    "      f'y_train: {y_train.shape} y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T15:07:58.813039Z",
     "start_time": "2021-05-05T15:07:58.799119Z"
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "tokenizer = text.Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, Dense, LSTM, Embedding\n",
    "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Sequential\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.preprocessing import text, sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(target).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(list(df['combined_text']))\n",
    "list_tokenized_headlines = tokenizer.texts_to_sequences(df['combined_text'])\n",
    "X_t = sequence.pad_sequences(list_tokenized_headlines, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "embedding_size = 128\n",
    "model.add(Embedding(20000, embedding_size))\n",
    "model.add(LSTM(25, return_sequences=True))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(41, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_t, y, epochs=3, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "358.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
