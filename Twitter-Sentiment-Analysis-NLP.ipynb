{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T20:22:54.144125Z",
     "start_time": "2021-05-08T20:22:50.367744Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import libraries required to load, transform, analyze and plot data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(context='paper', style='darkgrid', \n",
    "        rc={'figure.facecolor':'white'}, font_scale=1.2)\n",
    "\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "from spacy.tokenizer import _get_regex_pattern\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, LeakyReLU\n",
    "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D, SpatialDropout1D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import keras.backend as k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T20:22:54.150241Z",
     "start_time": "2021-05-08T20:22:54.145526Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove scientific notation and restrictions on df rows/columns display\n",
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('max_rows', 200)\n",
    "pd.set_option('display.max_colwidth', 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T20:22:54.343986Z",
     "start_time": "2021-05-08T20:22:54.302123Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>directed_at</th>\n",
       "      <th>emotion_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/iPhone app that you'll likely appreciate for its design. Also, they're giving free Ts at #SXSW</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as crashy as this year's iPhone app. #sxsw</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp;amp; Matt Mullenweg (Wordpress)</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                    tweet_text  \\\n",
       "0              .@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/iPhone app that you'll likely appreciate for its design. Also, they're giving free Ts at #SXSW   \n",
       "2                                                              @swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.   \n",
       "3                                                           @sxsw I hope this year's festival isn't as crashy as this year's iPhone app. #sxsw   \n",
       "4          @sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp; Matt Mullenweg (Wordpress)   \n",
       "\n",
       "          directed_at     emotion_label  \n",
       "0              iPhone  Negative emotion  \n",
       "1  iPad or iPhone App  Positive emotion  \n",
       "2                iPad  Positive emotion  \n",
       "3  iPad or iPhone App  Negative emotion  \n",
       "4              Google  Positive emotion  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load primary source file to df, renaming columns, dropping non-ASCII\n",
    "col_names = ['tweet_text', 'directed_at', 'emotion_label']\n",
    "tweets = pd.read_csv('data/judge-1377884607_tweet_product_company.csv', encoding= 'unicode_escape', names=col_names, header=0)\n",
    "tweets.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T20:22:56.058589Z",
     "start_time": "2021-05-08T20:22:56.034824Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9093 entries, 0 to 9092\n",
      "Data columns (total 3 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   tweet_text     9092 non-null   object\n",
      " 1   directed_at    3291 non-null   object\n",
      " 2   emotion_label  9093 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 213.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# review data types and null counts\n",
    "tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T20:22:58.524981Z",
     "start_time": "2021-05-08T20:22:58.505734Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9092, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop nan tweets from dataframe\n",
    "tweets.dropna(subset = ['tweet_text'], inplace=True)\n",
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T20:22:59.734452Z",
     "start_time": "2021-05-08T20:22:59.720981Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN                               0.638\n",
      "iPad                              0.104\n",
      "Apple                             0.073\n",
      "iPad or iPhone App                0.052\n",
      "Google                            0.047\n",
      "iPhone                            0.033\n",
      "Other Google product or service   0.032\n",
      "Android App                       0.009\n",
      "Android                           0.009\n",
      "Other Apple product or service    0.004\n",
      "Name: directed_at, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# check value counts by column\n",
    "print(tweets['directed_at'].value_counts(normalize=True, dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T20:23:01.110155Z",
     "start_time": "2021-05-08T20:23:01.096076Z"
    }
   },
   "outputs": [],
   "source": [
    "# create brand feature\n",
    "tweets['directed_at'].fillna('None', inplace=True)\n",
    "brand_map = {'iPad': 'Apple', 'Apple': 'Apple', 'iPad or iPhone App': 'Apple', \n",
    "             'Google': 'Google', 'iPhone': 'Apple', \n",
    "             'Other Google product or service': 'Google',\n",
    "            'Android App': 'Google', 'Android': 'Google',\n",
    "             'Other Apple product or service': 'Apple',\n",
    "             'None': 'None'\n",
    "            }\n",
    "tweets['brand'] = tweets.directed_at.map(brand_map, na_action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T20:23:02.809471Z",
     "start_time": "2021-05-08T20:23:02.790986Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral    0.593\n",
      "Positive   0.328\n",
      "Negative   0.063\n",
      "Unknown    0.017\n",
      "Name: emotion_label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# clean emotion labels\n",
    "tweets['emotion_label'].replace({'No emotion toward brand or product': 'Neutral',\n",
    "                                 'Positive emotion': 'Positive', \n",
    "                                 'Negative emotion': 'Negative', \n",
    "                                 'I can\\'t tell': 'Unknown'}, inplace=True)\n",
    "\n",
    "# check value counts by column\n",
    "print(tweets['emotion_label'].value_counts(normalize=True, dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T20:23:03.453541Z",
     "start_time": "2021-05-08T20:23:03.434450Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "brand   emotion_label\n",
       "Apple   Negative          388\n",
       "        Neutral            65\n",
       "        Positive         1949\n",
       "        Unknown             7\n",
       "Google  Negative          131\n",
       "        Neutral            26\n",
       "        Positive          723\n",
       "        Unknown             2\n",
       "None    Negative           51\n",
       "        Neutral          5297\n",
       "        Positive          306\n",
       "        Unknown           147\n",
       "Name: tweet_text, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check value counts by column\n",
    "tweets.groupby(by=['brand', 'emotion_label'])['tweet_text'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Text Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T20:23:05.883431Z",
     "start_time": "2021-05-08T20:23:05.876656Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(x):\n",
    "    \"\"\"\n",
    "    Helper function to remove punctuation from a string x: any string\n",
    "    \"\"\"\n",
    "    punctuation = set(string.punctuation) # punctuation of English language\n",
    "    punctuation.remove('#') # remove # so hashtags remain in x\n",
    "\n",
    "    x = re.sub('@[A-Za-z0-9]+', '', x) # remove @mention users\n",
    "    x = re.sub(r'http\\S+', '', x) # remove URL references\n",
    "    x = re.sub(r'\\b[0-9]+\\b', '', x) # remove stand-alone numbers  \n",
    "    x = ''.join(ch for ch in x if ch not in punctuation) # remove punctuation\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T20:23:07.430386Z",
     "start_time": "2021-05-08T20:23:07.425518Z"
    }
   },
   "outputs": [],
   "source": [
    "# function to clean text\n",
    "def  clean_text(df, text_field, new_text_field):\n",
    "    df[new_text_field] = df[text_field].str.lower()\n",
    "    df[new_text_field] = df[new_text_field].apply(remove_punctuation) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T20:23:08.634681Z",
     "start_time": "2021-05-08T20:23:08.481040Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>directed_at</th>\n",
       "      <th>emotion_label</th>\n",
       "      <th>brand</th>\n",
       "      <th>tweet_text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Apple</td>\n",
       "      <td>i have a 3g iphone after  hrs tweeting at #riseaustin it was dead  i need to upgrade plugin stations at #sxsw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/iPhone app that you'll likely appreciate for its design. Also, they're giving free Ts at #SXSW</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Apple</td>\n",
       "      <td>know about   awesome ipadiphone app that youll likely appreciate for its design also theyre giving free ts at #sxsw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Apple</td>\n",
       "      <td>can not wait for #ipad  also they should sale them down at #sxsw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as crashy as this year's iPhone app. #sxsw</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Apple</td>\n",
       "      <td>i hope this years festival isnt as crashy as this years iphone app #sxsw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp;amp; Matt Mullenweg (Wordpress)</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Google</td>\n",
       "      <td>great stuff on fri #sxsw marissa mayer google tim oreilly tech booksconferences amp matt mullenweg wordpress</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                    tweet_text  \\\n",
       "0              .@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/iPhone app that you'll likely appreciate for its design. Also, they're giving free Ts at #SXSW   \n",
       "2                                                              @swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.   \n",
       "3                                                           @sxsw I hope this year's festival isn't as crashy as this year's iPhone app. #sxsw   \n",
       "4          @sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp; Matt Mullenweg (Wordpress)   \n",
       "\n",
       "          directed_at emotion_label   brand  \\\n",
       "0              iPhone      Negative   Apple   \n",
       "1  iPad or iPhone App      Positive   Apple   \n",
       "2                iPad      Positive   Apple   \n",
       "3  iPad or iPhone App      Negative   Apple   \n",
       "4              Google      Positive  Google   \n",
       "\n",
       "                                                                                                       tweet_text_clean  \n",
       "0         i have a 3g iphone after  hrs tweeting at #riseaustin it was dead  i need to upgrade plugin stations at #sxsw  \n",
       "1   know about   awesome ipadiphone app that youll likely appreciate for its design also theyre giving free ts at #sxsw  \n",
       "2                                                      can not wait for #ipad  also they should sale them down at #sxsw  \n",
       "3                                              i hope this years festival isnt as crashy as this years iphone app #sxsw  \n",
       "4          great stuff on fri #sxsw marissa mayer google tim oreilly tech booksconferences amp matt mullenweg wordpress  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_clean = clean_text(tweets, 'tweet_text', 'tweet_text_clean')\n",
    "tweets_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T20:23:11.743831Z",
     "start_time": "2021-05-08T20:23:10.742976Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "nlp = en_core_web_sm.load()\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# get default pattern for tokens that don't get split\n",
    "re_token_match = _get_regex_pattern(nlp.Defaults.token_match)\n",
    "# add your patterns (here: hashtags and in-word hyphens)\n",
    "re_token_match = f\"({re_token_match}|#\\w+|\\w+-\\w+)\"\n",
    "\n",
    "# overwrite token_match function of the tokenizer\n",
    "nlp.tokenizer.token_match = re.compile(re_token_match).match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T20:29:04.933618Z",
     "start_time": "2021-05-08T20:23:14.593935Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy Stopword Count: 326\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>directed_at</th>\n",
       "      <th>emotion_label</th>\n",
       "      <th>brand</th>\n",
       "      <th>tweet_text_clean</th>\n",
       "      <th>tokens_sp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Apple</td>\n",
       "      <td>i have a 3g iphone after  hrs tweeting at #riseaustin it was dead  i need to upgrade plugin stations at #sxsw</td>\n",
       "      <td>[g, iphone, hrs, tweet, #riseaustin, dead, need, upgrade, plugin, station, #sxsw]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/iPhone app that you'll likely appreciate for its design. Also, they're giving free Ts at #SXSW</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Apple</td>\n",
       "      <td>know about   awesome ipadiphone app that youll likely appreciate for its design also theyre giving free ts at #sxsw</td>\n",
       "      <td>[know, awesome, ipadiphone, app, will, likely, appreciate, design, give, free, ts, #sxsw]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Apple</td>\n",
       "      <td>can not wait for #ipad  also they should sale them down at #sxsw</td>\n",
       "      <td>[wait, #ipad, sale, #sxsw]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as crashy as this year's iPhone app. #sxsw</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Apple</td>\n",
       "      <td>i hope this years festival isnt as crashy as this years iphone app #sxsw</td>\n",
       "      <td>[hope, year, festival, not, crashy, year, iphone, app, #sxsw]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp;amp; Matt Mullenweg (Wordpress)</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Google</td>\n",
       "      <td>great stuff on fri #sxsw marissa mayer google tim oreilly tech booksconferences amp matt mullenweg wordpress</td>\n",
       "      <td>[great, stuff, fri, #sxsw, marissa, mayer, google, tim, oreilly, tech, booksconference, amp, matt, mullenweg, wordpress]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                    tweet_text  \\\n",
       "0              .@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/iPhone app that you'll likely appreciate for its design. Also, they're giving free Ts at #SXSW   \n",
       "2                                                              @swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.   \n",
       "3                                                           @sxsw I hope this year's festival isn't as crashy as this year's iPhone app. #sxsw   \n",
       "4          @sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp; Matt Mullenweg (Wordpress)   \n",
       "\n",
       "          directed_at emotion_label   brand  \\\n",
       "0              iPhone      Negative   Apple   \n",
       "1  iPad or iPhone App      Positive   Apple   \n",
       "2                iPad      Positive   Apple   \n",
       "3  iPad or iPhone App      Negative   Apple   \n",
       "4              Google      Positive  Google   \n",
       "\n",
       "                                                                                                       tweet_text_clean  \\\n",
       "0         i have a 3g iphone after  hrs tweeting at #riseaustin it was dead  i need to upgrade plugin stations at #sxsw   \n",
       "1   know about   awesome ipadiphone app that youll likely appreciate for its design also theyre giving free ts at #sxsw   \n",
       "2                                                      can not wait for #ipad  also they should sale them down at #sxsw   \n",
       "3                                              i hope this years festival isnt as crashy as this years iphone app #sxsw   \n",
       "4          great stuff on fri #sxsw marissa mayer google tim oreilly tech booksconferences amp matt mullenweg wordpress   \n",
       "\n",
       "                                                                                                                  tokens_sp  \n",
       "0                                         [g, iphone, hrs, tweet, #riseaustin, dead, need, upgrade, plugin, station, #sxsw]  \n",
       "1                                 [know, awesome, ipadiphone, app, will, likely, appreciate, design, give, free, ts, #sxsw]  \n",
       "2                                                                                                [wait, #ipad, sale, #sxsw]  \n",
       "3                                                             [hope, year, festival, not, crashy, year, iphone, app, #sxsw]  \n",
       "4  [great, stuff, fri, #sxsw, marissa, mayer, google, tim, oreilly, tech, booksconference, amp, matt, mullenweg, wordpress]  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "stops_sp = nlp.Defaults.stop_words\n",
    "print(f'spaCy Stopword Count: {len(stops_sp)}')\n",
    "\n",
    "def clean_token(doc):\n",
    "    return [token.lemma_ for token in doc if not token.is_stop \n",
    "            and not token.is_punct and not token.is_digit \n",
    "            and not token.is_space]\n",
    "\n",
    "tweets['tokens_sp'] = [clean_token(nlp(row)) for row in tweets.tweet_text_clean.apply(str)]\n",
    "\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T18:02:24.449474Z",
     "start_time": "2021-05-08T18:02:24.420635Z"
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "word_dict = {}\n",
    "\n",
    "# Loop through all the tags\n",
    "for i, row in tweets['tokens_sp'].iteritems():\n",
    "    for word in row:\n",
    "        if word not in word_dict:\n",
    "            word_dict[word] = 1\n",
    "        else:\n",
    "            word_dict[word] +=1\n",
    "\n",
    "word_counts = sorted(word_dict.items(), key=lambda kv: kv[1], reverse=True)\n",
    "print(f'Total words: {len(word_counts)}')\n",
    "word_counts[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T20:29:04.943370Z",
     "start_time": "2021-05-08T20:29:04.934921Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0           i have a 3g iphone after  hrs tweeting at #riseaustin it was dead  i need to upgrade plugin stations at #sxsw\n",
      "1     know about   awesome ipadiphone app that youll likely appreciate for its design also theyre giving free ts at #sxsw\n",
      "2                                                        can not wait for #ipad  also they should sale them down at #sxsw\n",
      "Name: tweet_text_clean, dtype: object    Negative  Neutral  Positive\n",
      "0         1        0         0\n",
      "1         0        0         1\n",
      "2         0        0         1\n"
     ]
    }
   ],
   "source": [
    "# filter tweets for identifiable emotions only (drop unknown)\n",
    "sentiments = ['Positive', 'Negative', 'Neutral']\n",
    "tweets_f = tweets[tweets['emotion_label'].isin(sentiments)]\n",
    "\n",
    "# create X and y (one-hot encoded for 3 classes)\n",
    "Xt = tweets_f['tokens_sp']\n",
    "X = tweets_f['tweet_text_clean']\n",
    "y = pd.get_dummies(tweets_f['emotion_label'])\n",
    "print(X.iloc[:3], y.iloc[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T20:58:51.611430Z",
     "start_time": "2021-05-08T20:58:51.599819Z"
    }
   },
   "outputs": [],
   "source": [
    "def TrainTestSplit(X, y):\n",
    "    # keras tokenize sequences with padding\n",
    "    t = text.Tokenizer(oov_token=1)\n",
    "    t.fit_on_texts(X)\n",
    "    X_seq = t.texts_to_sequences(X)\n",
    "    X_seq_pad = sequence.pad_sequences(X_seq, padding='post') \n",
    "   \n",
    "    # Split into training and test sets for cleaned text\n",
    "    SEED = 19\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_seq_pad, y, test_size=0.20, stratify=y, random_state=SEED)\n",
    "\n",
    "    print(f'X_train: {X_train.shape} X_test: {X_test.shape} ' \n",
    "          f'y_train: {y_train.shape} y_test: {y_test.shape}')\n",
    "\n",
    "    # set parameters for model input\n",
    "    doc_cnt = X_train.shape[0]\n",
    "    vocab_size = len(t.word_index)+1\n",
    "    input_length = X_train.shape[1]\n",
    "\n",
    "    print(f'Document count: {doc_cnt} | '\n",
    "          f'Vocab size: {vocab_size} | '\n",
    "          f'Input length: {input_length} '\n",
    "         )\n",
    "    return t, X_train, X_test, y_train, y_test, doc_cnt, vocab_size, input_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T20:58:53.881162Z",
     "start_time": "2021-05-08T20:58:53.563199Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (7148, 30) X_test: (1788, 30) y_train: (7148, 3) y_test: (1788, 3)\n",
      "Document count: 7148 | Vocab size: 10565 | Input length: 30 \n"
     ]
    }
   ],
   "source": [
    "#\n",
    "X_tok, X_train, X_test, y_train, y_test, doc_cnt, vocab_size, input_length = TrainTestSplit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T20:59:00.968593Z",
     "start_time": "2021-05-08T20:59:00.782840Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (7148, 23) X_test: (1788, 23) y_train: (7148, 3) y_test: (1788, 3)\n",
      "Document count: 7148 | Vocab size: 9435 | Input length: 23 \n"
     ]
    }
   ],
   "source": [
    "#\n",
    "Xt_tok, Xt_train, Xt_test, yt_train, yt_test, doc_cnt2, vocab_size2, \\\n",
    "    input_length2 = TrainTestSplit(Xt, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## m1 - LSTM Embed Tweets Cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T20:47:38.624842Z",
     "start_time": "2021-05-08T20:47:38.463177Z"
    }
   },
   "outputs": [],
   "source": [
    "# instantiate keras sequential LSTM model layers with embeddings\n",
    "m1 = Sequential(name=\"m1_seq_lstm\")\n",
    "m1.add(Embedding(input_dim=vocab_size, \n",
    "                    output_dim=128, \n",
    "                    input_length=input_length\n",
    "                    ))\n",
    "m1.add(LSTM(64, return_sequences=True))\n",
    "m1.add(GlobalMaxPool1D()) # downsamples input by taking the maximum value over the time dimension\n",
    "m1.add(Dropout(0.15)) # drop out lower or remove (regularization)\n",
    "m1.add(Dense(32, activation='relu'))\n",
    "m1.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T20:47:39.742724Z",
     "start_time": "2021-05-08T20:47:39.684242Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"m1_seq_lstm\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 30, 128)           1352320   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 30, 64)            49408     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_4 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 1,403,907\n",
      "Trainable params: 1,403,907\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# compile model and print summary\n",
    "m1.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "m1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T20:48:40.440069Z",
     "start_time": "2021-05-08T20:47:41.808765Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cutterback/opt/anaconda3/envs/p37env/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6075 samples, validate on 1073 samples\n",
      "Epoch 1/50\n",
      "6075/6075 [==============================] - 13s 2ms/step - loss: 0.8644 - accuracy: 0.6028 - val_loss: 0.8073 - val_accuracy: 0.5993\n",
      "Epoch 2/50\n",
      "6075/6075 [==============================] - 11s 2ms/step - loss: 0.7143 - accuracy: 0.6935 - val_loss: 0.7444 - val_accuracy: 0.6608\n",
      "Epoch 3/50\n",
      "6075/6075 [==============================] - 11s 2ms/step - loss: 0.5566 - accuracy: 0.7822 - val_loss: 0.7546 - val_accuracy: 0.6803\n",
      "Epoch 4/50\n",
      "6075/6075 [==============================] - 11s 2ms/step - loss: 0.4379 - accuracy: 0.8333 - val_loss: 0.8360 - val_accuracy: 0.6449\n",
      "Epoch 5/50\n",
      "6075/6075 [==============================] - 11s 2ms/step - loss: 0.3209 - accuracy: 0.8765 - val_loss: 0.8593 - val_accuracy: 0.6859\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fb732f94090>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the callbacks, early stopping and save final model\n",
    "early_stop_m1 = [EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True), \n",
    "                  ModelCheckpoint(filepath='best_model_m1.h5', monitor='val_loss',\n",
    "                                  save_best_only=True)]\n",
    "\n",
    "m1.fit(X_train, y_train, epochs=50, callbacks=early_stop_m1, \n",
    "       validation_split=0.15, shuffle=True, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## m2 - LSTM Embed Tweet Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T20:48:55.408369Z",
     "start_time": "2021-05-08T20:48:55.250197Z"
    }
   },
   "outputs": [],
   "source": [
    "# instantiate keras sequential LSTM model layers with embeddings\n",
    "model = Sequential(name=\"m2_seq_lstm\")\n",
    "model.add(Embedding(input_dim=vocab_size2, \n",
    "                    output_dim=64, \n",
    "                    input_length=input_length2\n",
    "                    ))\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(GlobalMaxPool1D()) # downsamples input by taking the maximum value over the time dimension\n",
    "model.add(Dropout(0.15)) # drop out lower or remove (regularization)\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T20:49:00.837469Z",
     "start_time": "2021-05-08T20:49:00.775250Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"m2_seq_lstm\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 23, 64)            603840    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 23, 64)            33024     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_5 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 639,043\n",
      "Trainable params: 639,043\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# compile model and print summary\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T20:50:30.604403Z",
     "start_time": "2021-05-08T20:49:48.420058Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cutterback/opt/anaconda3/envs/p37env/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6075 samples, validate on 1073 samples\n",
      "Epoch 1/50\n",
      "6075/6075 [==============================] - 9s 2ms/step - loss: 0.8824 - accuracy: 0.6035 - val_loss: 0.8297 - val_accuracy: 0.5983\n",
      "Epoch 2/50\n",
      "6075/6075 [==============================] - 8s 1ms/step - loss: 0.7407 - accuracy: 0.6754 - val_loss: 0.7377 - val_accuracy: 0.6533\n",
      "Epoch 3/50\n",
      "6075/6075 [==============================] - 8s 1ms/step - loss: 0.5305 - accuracy: 0.7834 - val_loss: 0.7393 - val_accuracy: 0.6813\n",
      "Epoch 4/50\n",
      "6075/6075 [==============================] - 8s 1ms/step - loss: 0.3870 - accuracy: 0.8540 - val_loss: 0.8238 - val_accuracy: 0.6580\n",
      "Epoch 5/50\n",
      "6075/6075 [==============================] - 8s 1ms/step - loss: 0.3059 - accuracy: 0.8830 - val_loss: 0.9182 - val_accuracy: 0.6785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fb797096950>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the callbacks, early stopping and save final model\n",
    "early_stop = [EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True), \n",
    "                  ModelCheckpoint(filepath='best_model_m2.h5', monitor='val_loss',\n",
    "                                  save_best_only=True)]\n",
    "\n",
    "model.fit(Xt_train, yt_train, epochs=50, callbacks=early_stop,\n",
    "          validation_split=0.15, shuffle=True, batch_size=64) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes \n",
    "- Visualize n-grams or word clouts or frequency association\n",
    "- Word embeddings - Glove vectors\n",
    "- Encode with Bag of Words/TGIDF with RF (helps with interpretability) - helps link complexity; still could use NN (basic dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## m3 - LSTM Embed Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T20:51:30.903528Z",
     "start_time": "2021-05-08T20:51:30.733759Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tweet vocabulary is: 10563\n"
     ]
    }
   ],
   "source": [
    "# establish total vocabulary prior to fetching Glove vectors\n",
    "t0 = text.Tokenizer(oov_token=1)\n",
    "t0.fit_on_texts(X)\n",
    "vocab = list(t0.word_counts.keys())\n",
    "print(f'Total tweet vocabulary is: {len(vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T20:54:10.243063Z",
     "start_time": "2021-05-08T20:51:33.813866Z"
    }
   },
   "outputs": [],
   "source": [
    "# create glove dictionary of embedding vectors for vocab list\n",
    "glove_file = 'data/glove.twitter.27B.200d.txt'\n",
    "glove = {}\n",
    "with open(glove_file, 'rb') as f:\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode('utf-8')\n",
    "        if word in vocab:\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            glove[word] = vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T20:59:18.345504Z",
     "start_time": "2021-05-08T20:59:18.304988Z"
    }
   },
   "outputs": [],
   "source": [
    "# Number of dimensions of the GloVe word embeddings\n",
    "glove_dim = len(next(iter(glove.values())))  \n",
    "\n",
    "emb_matrix = np.zeros((vocab_size, glove_dim))\n",
    "for w, i in X_tok.word_index.items():\n",
    "    vect = glove.get(w)\n",
    "    if vect is not None:\n",
    "        emb_matrix[i] = vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T20:59:22.025446Z",
     "start_time": "2021-05-08T20:59:21.426436Z"
    }
   },
   "outputs": [],
   "source": [
    "# instantiate keras sequential LSTM model layers with embeddings\n",
    "m3 = Sequential(name=\"m3_seq_lstm_glove\")\n",
    "m3.add(Embedding(input_dim=vocab_size, \n",
    "                 output_dim=glove_dim, \n",
    "                 input_length=input_length,\n",
    "                 weights=[emb_matrix], \n",
    "                 trainable=False\n",
    "                    ))\n",
    "m3.add(SpatialDropout1D(0.2))\n",
    "m3.add(LSTM(128, return_sequences=True))\n",
    "m3.add(Bidirectional(LSTM(128, dropout=0.15, recurrent_dropout=0.15, return_sequences=True)))\n",
    "m3.add(GlobalMaxPool1D()) # downsamples input by taking the maximum value over the time dimension\n",
    "m3.add(Dense(64, activation=\"relu\")) \n",
    "m3.add(Dense(32, activation=\"relu\"))\n",
    "m3.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T20:59:24.187629Z",
     "start_time": "2021-05-08T20:59:24.128424Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"m3_seq_lstm_glove\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 30, 200)           2113000   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 30, 200)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 30, 128)           168448    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 30, 256)           263168    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_6 (Glob (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 2,563,243\n",
      "Trainable params: 450,243\n",
      "Non-trainable params: 2,113,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# compile model and print summary\n",
    "opt = optimizers.SGD(lr=0.001, momentum=0.95) \n",
    "m3.compile(loss='categorical_crossentropy', optimizer=opt, \n",
    "              metrics=['accuracy'])\n",
    "m3.summary()\n",
    "# RMSprop(learning_rate=0.001, rho=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T21:22:47.293810Z",
     "start_time": "2021-05-08T20:59:27.302672Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6075 samples, validate on 1073 samples\n",
      "Epoch 1/50\n",
      "6075/6075 [==============================] - 36s 6ms/step - loss: 0.9688 - accuracy: 0.5661 - val_loss: 0.8801 - val_accuracy: 0.5983\n",
      "Epoch 2/50\n",
      "6075/6075 [==============================] - 33s 5ms/step - loss: 0.8621 - accuracy: 0.6038 - val_loss: 0.8554 - val_accuracy: 0.5983\n",
      "Epoch 3/50\n",
      "6075/6075 [==============================] - 32s 5ms/step - loss: 0.8499 - accuracy: 0.6038 - val_loss: 0.8497 - val_accuracy: 0.5983\n",
      "Epoch 4/50\n",
      "6075/6075 [==============================] - 32s 5ms/step - loss: 0.8456 - accuracy: 0.6038 - val_loss: 0.8467 - val_accuracy: 0.5983\n",
      "Epoch 5/50\n",
      "6075/6075 [==============================] - 32s 5ms/step - loss: 0.8429 - accuracy: 0.6038 - val_loss: 0.8437 - val_accuracy: 0.5983\n",
      "Epoch 6/50\n",
      "6075/6075 [==============================] - 32s 5ms/step - loss: 0.8400 - accuracy: 0.6038 - val_loss: 0.8413 - val_accuracy: 0.5983\n",
      "Epoch 7/50\n",
      "6075/6075 [==============================] - 32s 5ms/step - loss: 0.8381 - accuracy: 0.6038 - val_loss: 0.8373 - val_accuracy: 0.5983\n",
      "Epoch 8/50\n",
      "6075/6075 [==============================] - 32s 5ms/step - loss: 0.8352 - accuracy: 0.6038 - val_loss: 0.8337 - val_accuracy: 0.5983\n",
      "Epoch 9/50\n",
      "6075/6075 [==============================] - 32s 5ms/step - loss: 0.8316 - accuracy: 0.6038 - val_loss: 0.8298 - val_accuracy: 0.5983\n",
      "Epoch 10/50\n",
      "6075/6075 [==============================] - 39s 6ms/step - loss: 0.8279 - accuracy: 0.6038 - val_loss: 0.8249 - val_accuracy: 0.5983\n",
      "Epoch 11/50\n",
      "6075/6075 [==============================] - 41s 7ms/step - loss: 0.8229 - accuracy: 0.6038 - val_loss: 0.8206 - val_accuracy: 0.5983\n",
      "Epoch 12/50\n",
      "6075/6075 [==============================] - 44s 7ms/step - loss: 0.8194 - accuracy: 0.6038 - val_loss: 0.8136 - val_accuracy: 0.5983\n",
      "Epoch 13/50\n",
      "6075/6075 [==============================] - 33s 5ms/step - loss: 0.8140 - accuracy: 0.6038 - val_loss: 0.8084 - val_accuracy: 0.5983\n",
      "Epoch 14/50\n",
      "6075/6075 [==============================] - 32s 5ms/step - loss: 0.8070 - accuracy: 0.6053 - val_loss: 0.8007 - val_accuracy: 0.5955\n",
      "Epoch 15/50\n",
      "6075/6075 [==============================] - 32s 5ms/step - loss: 0.8024 - accuracy: 0.6051 - val_loss: 0.7956 - val_accuracy: 0.6048\n",
      "Epoch 16/50\n",
      "6075/6075 [==============================] - 32s 5ms/step - loss: 0.7985 - accuracy: 0.6064 - val_loss: 0.7961 - val_accuracy: 0.5927\n",
      "Epoch 17/50\n",
      "6075/6075 [==============================] - 32s 5ms/step - loss: 0.7938 - accuracy: 0.6082 - val_loss: 0.7842 - val_accuracy: 0.6067\n",
      "Epoch 18/50\n",
      "6075/6075 [==============================] - 32s 5ms/step - loss: 0.7918 - accuracy: 0.6133 - val_loss: 0.7984 - val_accuracy: 0.6021\n",
      "Epoch 19/50\n",
      "6075/6075 [==============================] - 32s 5ms/step - loss: 0.7880 - accuracy: 0.6181 - val_loss: 0.7753 - val_accuracy: 0.6170\n",
      "Epoch 20/50\n",
      "6075/6075 [==============================] - 32s 5ms/step - loss: 0.7840 - accuracy: 0.6168 - val_loss: 0.7850 - val_accuracy: 0.5965\n",
      "Epoch 21/50\n",
      "6075/6075 [==============================] - 32s 5ms/step - loss: 0.7871 - accuracy: 0.6209 - val_loss: 0.7693 - val_accuracy: 0.6198\n",
      "Epoch 22/50\n",
      "6075/6075 [==============================] - 32s 5ms/step - loss: 0.7781 - accuracy: 0.6291 - val_loss: 0.7654 - val_accuracy: 0.6244\n",
      "Epoch 23/50\n",
      "6075/6075 [==============================] - 32s 5ms/step - loss: 0.7759 - accuracy: 0.6258 - val_loss: 0.7693 - val_accuracy: 0.6253\n",
      "Epoch 24/50\n",
      "6075/6075 [==============================] - 32s 5ms/step - loss: 0.7853 - accuracy: 0.6214 - val_loss: 0.7648 - val_accuracy: 0.6207\n",
      "Epoch 25/50\n",
      "6075/6075 [==============================] - 32s 5ms/step - loss: 0.7759 - accuracy: 0.6249 - val_loss: 0.7572 - val_accuracy: 0.6291\n",
      "Epoch 26/50\n",
      "6075/6075 [==============================] - 32s 5ms/step - loss: 0.7723 - accuracy: 0.6252 - val_loss: 0.7818 - val_accuracy: 0.6067\n",
      "Epoch 27/50\n",
      "6075/6075 [==============================] - 32s 5ms/step - loss: 0.7687 - accuracy: 0.6305 - val_loss: 0.7508 - val_accuracy: 0.6365\n",
      "Epoch 28/50\n",
      "6075/6075 [==============================] - 32s 5ms/step - loss: 0.7740 - accuracy: 0.6288 - val_loss: 0.7530 - val_accuracy: 0.6328\n",
      "Epoch 29/50\n",
      "6075/6075 [==============================] - 32s 5ms/step - loss: 0.7658 - accuracy: 0.6278 - val_loss: 0.8042 - val_accuracy: 0.5871\n",
      "Epoch 30/50\n",
      "6075/6075 [==============================] - 33s 5ms/step - loss: 0.7646 - accuracy: 0.6365 - val_loss: 0.7440 - val_accuracy: 0.6412\n",
      "Epoch 31/50\n",
      "6075/6075 [==============================] - 33s 5ms/step - loss: 0.7602 - accuracy: 0.6418 - val_loss: 0.7420 - val_accuracy: 0.6412\n",
      "Epoch 32/50\n",
      "6075/6075 [==============================] - 32s 5ms/step - loss: 0.7533 - accuracy: 0.6472 - val_loss: 0.7386 - val_accuracy: 0.6384\n",
      "Epoch 33/50\n",
      "6075/6075 [==============================] - 32s 5ms/step - loss: 0.7587 - accuracy: 0.6372 - val_loss: 0.7347 - val_accuracy: 0.6552\n",
      "Epoch 34/50\n",
      "6075/6075 [==============================] - 33s 5ms/step - loss: 0.7541 - accuracy: 0.6446 - val_loss: 0.7312 - val_accuracy: 0.6524\n",
      "Epoch 35/50\n",
      "6075/6075 [==============================] - 32s 5ms/step - loss: 0.7504 - accuracy: 0.6458 - val_loss: 0.7293 - val_accuracy: 0.6570\n",
      "Epoch 36/50\n",
      "6075/6075 [==============================] - 32s 5ms/step - loss: 0.7452 - accuracy: 0.6458 - val_loss: 0.7328 - val_accuracy: 0.6421\n",
      "Epoch 37/50\n",
      "6075/6075 [==============================] - 34s 6ms/step - loss: 0.7467 - accuracy: 0.6474 - val_loss: 0.7275 - val_accuracy: 0.6403\n",
      "Epoch 38/50\n",
      "6075/6075 [==============================] - 34s 6ms/step - loss: 0.7522 - accuracy: 0.6421 - val_loss: 0.7431 - val_accuracy: 0.6440\n",
      "Epoch 39/50\n",
      "6075/6075 [==============================] - 34s 6ms/step - loss: 0.7453 - accuracy: 0.6459 - val_loss: 0.7212 - val_accuracy: 0.6496\n",
      "Epoch 40/50\n",
      "6075/6075 [==============================] - 33s 5ms/step - loss: 0.7405 - accuracy: 0.6504 - val_loss: 0.7324 - val_accuracy: 0.6393\n",
      "Epoch 41/50\n",
      "6075/6075 [==============================] - 33s 5ms/step - loss: 0.7400 - accuracy: 0.6517 - val_loss: 0.7243 - val_accuracy: 0.6701\n",
      "Epoch 42/50\n",
      "6075/6075 [==============================] - 34s 6ms/step - loss: 0.7284 - accuracy: 0.6578 - val_loss: 0.7315 - val_accuracy: 0.6496\n"
     ]
    }
   ],
   "source": [
    "# Define the callbacks, early stopping and save final model\n",
    "early_stop_m3 = [EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True), \n",
    "                  ModelCheckpoint(filepath='best_model_m3.h5', monitor='val_loss',\n",
    "                                  save_best_only=True)]\n",
    "\n",
    "# optimizer=RMSprop(lr=learning_rate)\n",
    "m3_fit = m3.fit(X_train, y_train, epochs=50, callbacks=early_stop_m3,\n",
    "          validation_split=0.15, shuffle=True, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T14:11:33.908828Z",
     "start_time": "2021-05-08T14:11:33.900938Z"
    }
   },
   "outputs": [],
   "source": [
    "m3_hist = m3_fit.history\n",
    "m3_hist.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T14:18:59.734340Z",
     "start_time": "2021-05-08T14:18:59.191673Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T14:11:35.936323Z",
     "start_time": "2021-05-08T14:11:35.928293Z"
    }
   },
   "outputs": [],
   "source": [
    "print(k.eval(m3.optimizer.lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output (probability) predictions for the test set \n",
    "y_hat_test = model.predict(test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the loss and accuracy for the training set \n",
    "results_train = model.evaluate(train, label_train)\n",
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-08T14:11:37.194277Z",
     "start_time": "2021-05-08T14:11:37.008101Z"
    }
   },
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(m3_hist['accuracy'])\n",
    "plt.plot(m3_hist['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.load_weights(checkpoint_path)\n",
    "    predictions = m.predict(X_test, verbose=1)\n",
    "    print('Validation Loss:', log_loss(y_test, predictions))\n",
    "    print('Test Accuracy', (predictions.argmax(axis = 1) == y_test.argmax(axis = 1)).mean())\n",
    "    print('F1 Score:', f1_score(y_test.argmax(axis = 1), predictions.argmax(axis = 1), average='weighted'))\n",
    "    plot_confusion_matrix(y_test.argmax(axis = 1), predictions.argmax(axis = 1), classes=encoder.classes_)\n",
    "    plt.show()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:p37env] *",
   "language": "python",
   "name": "conda-env-p37env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "358.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
