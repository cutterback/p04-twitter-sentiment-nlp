{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T18:16:29.563040Z",
     "start_time": "2021-05-06T18:16:23.791717Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import libraries required to load, transform, analyze and plot data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(context='paper', style='darkgrid', \n",
    "        rc={'figure.facecolor':'white'}, font_scale=1.2)\n",
    "\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "from spacy.tokenizer import _get_regex_pattern\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, LSTM, Embedding\n",
    "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T18:16:29.571703Z",
     "start_time": "2021-05-06T18:16:29.564291Z"
    }
   },
   "outputs": [],
   "source": [
    "# remove scientific notation and restrictions on df rows/columns display\n",
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('max_rows', 200)\n",
    "pd.set_option('display.max_colwidth', 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T18:16:29.638858Z",
     "start_time": "2021-05-06T18:16:29.574787Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>directed_at</th>\n",
       "      <th>emotion_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/iPhone app that you'll likely appreciate for its design. Also, they're giving free Ts at #SXSW</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as crashy as this year's iPhone app. #sxsw</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp;amp; Matt Mullenweg (Wordpress)</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                    tweet_text  \\\n",
       "0              .@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/iPhone app that you'll likely appreciate for its design. Also, they're giving free Ts at #SXSW   \n",
       "2                                                              @swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.   \n",
       "3                                                           @sxsw I hope this year's festival isn't as crashy as this year's iPhone app. #sxsw   \n",
       "4          @sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp; Matt Mullenweg (Wordpress)   \n",
       "\n",
       "          directed_at     emotion_label  \n",
       "0              iPhone  Negative emotion  \n",
       "1  iPad or iPhone App  Positive emotion  \n",
       "2                iPad  Positive emotion  \n",
       "3  iPad or iPhone App  Negative emotion  \n",
       "4              Google  Positive emotion  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load primary source file to df, renaming columns, dropping non-ASCII\n",
    "col_names = ['tweet_text', 'directed_at', 'emotion_label']\n",
    "tweets = pd.read_csv('data/judge-1377884607_tweet_product_company.csv', encoding= 'unicode_escape', names=col_names, header=0)\n",
    "tweets.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T18:16:31.961036Z",
     "start_time": "2021-05-06T18:16:31.940351Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9093 entries, 0 to 9092\n",
      "Data columns (total 3 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   tweet_text     9092 non-null   object\n",
      " 1   directed_at    3291 non-null   object\n",
      " 2   emotion_label  9093 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 213.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# review data types and null counts\n",
    "tweets.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T18:16:34.733882Z",
     "start_time": "2021-05-06T18:16:34.722912Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9092, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop nan tweets from dataframe\n",
    "tweets.dropna(subset = ['tweet_text'], inplace=True)\n",
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T18:16:36.682710Z",
     "start_time": "2021-05-06T18:16:36.674261Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN                               0.638\n",
      "iPad                              0.104\n",
      "Apple                             0.073\n",
      "iPad or iPhone App                0.052\n",
      "Google                            0.047\n",
      "iPhone                            0.033\n",
      "Other Google product or service   0.032\n",
      "Android App                       0.009\n",
      "Android                           0.009\n",
      "Other Apple product or service    0.004\n",
      "Name: directed_at, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# check value counts by column\n",
    "print(tweets['directed_at'].value_counts(normalize=True, dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T18:16:38.819880Z",
     "start_time": "2021-05-06T18:16:38.807206Z"
    }
   },
   "outputs": [],
   "source": [
    "# create brand feature\n",
    "tweets['directed_at'].fillna('None', inplace=True)\n",
    "brand_map = {'iPad': 'Apple', 'Apple': 'Apple', 'iPad or iPhone App': 'Apple', \n",
    "             'Google': 'Google', 'iPhone': 'Apple', \n",
    "             'Other Google product or service': 'Google',\n",
    "            'Android App': 'Google', 'Android': 'Google',\n",
    "             'Other Apple product or service': 'Apple',\n",
    "             'None': 'None'\n",
    "            }\n",
    "tweets['brand'] = tweets.directed_at.map(brand_map, na_action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T18:16:40.749306Z",
     "start_time": "2021-05-06T18:16:40.739130Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neutral    0.593\n",
      "Positive   0.328\n",
      "Negative   0.063\n",
      "Unknown    0.017\n",
      "Name: emotion_label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# clean emotion labels\n",
    "tweets['emotion_label'].replace({'No emotion toward brand or product': 'Neutral',\n",
    "                                 'Positive emotion': 'Positive', \n",
    "                                 'Negative emotion': 'Negative', \n",
    "                                 'I can\\'t tell': 'Unknown'}, inplace=True)\n",
    "\n",
    "# check value counts by column\n",
    "print(tweets['emotion_label'].value_counts(normalize=True, dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T18:16:43.235020Z",
     "start_time": "2021-05-06T18:16:43.222877Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "brand   emotion_label\n",
       "Apple   Negative          388\n",
       "        Neutral            65\n",
       "        Positive         1949\n",
       "        Unknown             7\n",
       "Google  Negative          131\n",
       "        Neutral            26\n",
       "        Positive          723\n",
       "        Unknown             2\n",
       "None    Negative           51\n",
       "        Neutral          5297\n",
       "        Positive          306\n",
       "        Unknown           147\n",
       "Name: tweet_text, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check value counts by column\n",
    "tweets.groupby(by=['brand', 'emotion_label'])['tweet_text'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Text Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T18:16:45.265904Z",
     "start_time": "2021-05-06T18:16:45.253187Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(x):\n",
    "    \"\"\"\n",
    "    Helper function to remove punctuation from a string x: any string\n",
    "    \"\"\"\n",
    "    punctuation = set(string.punctuation) # punctuation of English language\n",
    "    punctuation.remove('#') # remove # so hashtags remain in x\n",
    "\n",
    "    x = re.sub('@[A-Za-z0-9]+', '', x) # remove @mention users\n",
    "    x = re.sub(r'http\\S+', '', x) # remove URL references\n",
    "    x = re.sub(r'\\b[0-9]+\\b', '', x) # remove stand-alone numbers  \n",
    "    x = ''.join(ch for ch in x if ch not in punctuation) # remove punctuation\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T18:16:47.287252Z",
     "start_time": "2021-05-06T18:16:47.281821Z"
    }
   },
   "outputs": [],
   "source": [
    "# function to clean text\n",
    "def  clean_text(df, text_field, new_text_field):\n",
    "    df[new_text_field] = df[text_field].str.lower()\n",
    "    df[new_text_field] = df[new_text_field].apply(remove_punctuation) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T18:16:48.498073Z",
     "start_time": "2021-05-06T18:16:48.287227Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>directed_at</th>\n",
       "      <th>emotion_label</th>\n",
       "      <th>brand</th>\n",
       "      <th>tweet_text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Apple</td>\n",
       "      <td>i have a 3g iphone after  hrs tweeting at #riseaustin it was dead  i need to upgrade plugin stations at #sxsw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/iPhone app that you'll likely appreciate for its design. Also, they're giving free Ts at #SXSW</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Apple</td>\n",
       "      <td>know about   awesome ipadiphone app that youll likely appreciate for its design also theyre giving free ts at #sxsw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Apple</td>\n",
       "      <td>can not wait for #ipad  also they should sale them down at #sxsw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as crashy as this year's iPhone app. #sxsw</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Apple</td>\n",
       "      <td>i hope this years festival isnt as crashy as this years iphone app #sxsw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp;amp; Matt Mullenweg (Wordpress)</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Google</td>\n",
       "      <td>great stuff on fri #sxsw marissa mayer google tim oreilly tech booksconferences amp matt mullenweg wordpress</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                    tweet_text  \\\n",
       "0              .@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/iPhone app that you'll likely appreciate for its design. Also, they're giving free Ts at #SXSW   \n",
       "2                                                              @swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.   \n",
       "3                                                           @sxsw I hope this year's festival isn't as crashy as this year's iPhone app. #sxsw   \n",
       "4          @sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp; Matt Mullenweg (Wordpress)   \n",
       "\n",
       "          directed_at emotion_label   brand  \\\n",
       "0              iPhone      Negative   Apple   \n",
       "1  iPad or iPhone App      Positive   Apple   \n",
       "2                iPad      Positive   Apple   \n",
       "3  iPad or iPhone App      Negative   Apple   \n",
       "4              Google      Positive  Google   \n",
       "\n",
       "                                                                                                       tweet_text_clean  \n",
       "0         i have a 3g iphone after  hrs tweeting at #riseaustin it was dead  i need to upgrade plugin stations at #sxsw  \n",
       "1   know about   awesome ipadiphone app that youll likely appreciate for its design also theyre giving free ts at #sxsw  \n",
       "2                                                      can not wait for #ipad  also they should sale them down at #sxsw  \n",
       "3                                              i hope this years festival isnt as crashy as this years iphone app #sxsw  \n",
       "4          great stuff on fri #sxsw marissa mayer google tim oreilly tech booksconferences amp matt mullenweg wordpress  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_clean = clean_text(tweets, 'tweet_text', 'tweet_text_clean')\n",
    "tweets_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T18:16:52.167536Z",
     "start_time": "2021-05-06T18:16:50.322241Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "nlp = en_core_web_sm.load()\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# get default pattern for tokens that don't get split\n",
    "re_token_match = _get_regex_pattern(nlp.Defaults.token_match)\n",
    "# add your patterns (here: hashtags and in-word hyphens)\n",
    "re_token_match = f\"({re_token_match}|#\\w+|\\w+-\\w+)\"\n",
    "\n",
    "# overwrite token_match function of the tokenizer\n",
    "nlp.tokenizer.token_match = re.compile(re_token_match).match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T18:23:10.141268Z",
     "start_time": "2021-05-06T18:16:53.516350Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy Stopword Count: 326\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>directed_at</th>\n",
       "      <th>emotion_label</th>\n",
       "      <th>brand</th>\n",
       "      <th>tweet_text_clean</th>\n",
       "      <th>tokens_sp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Apple</td>\n",
       "      <td>i have a 3g iphone after  hrs tweeting at #riseaustin it was dead  i need to upgrade plugin stations at #sxsw</td>\n",
       "      <td>[g, iphone, hrs, tweet, #riseaustin, dead, need, upgrade, plugin, station, #sxsw]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/iPhone app that you'll likely appreciate for its design. Also, they're giving free Ts at #SXSW</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Apple</td>\n",
       "      <td>know about   awesome ipadiphone app that youll likely appreciate for its design also theyre giving free ts at #sxsw</td>\n",
       "      <td>[know, awesome, ipadiphone, app, will, likely, appreciate, design, give, free, ts, #sxsw]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Apple</td>\n",
       "      <td>can not wait for #ipad  also they should sale them down at #sxsw</td>\n",
       "      <td>[wait, #ipad, sale, #sxsw]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as crashy as this year's iPhone app. #sxsw</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Apple</td>\n",
       "      <td>i hope this years festival isnt as crashy as this years iphone app #sxsw</td>\n",
       "      <td>[hope, year, festival, not, crashy, year, iphone, app, #sxsw]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp;amp; Matt Mullenweg (Wordpress)</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Google</td>\n",
       "      <td>great stuff on fri #sxsw marissa mayer google tim oreilly tech booksconferences amp matt mullenweg wordpress</td>\n",
       "      <td>[great, stuff, fri, #sxsw, marissa, mayer, google, tim, oreilly, tech, booksconference, amp, matt, mullenweg, wordpress]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                    tweet_text  \\\n",
       "0              .@wesley83 I have a 3G iPhone. After 3 hrs tweeting at #RISE_Austin, it was dead!  I need to upgrade. Plugin stations at #SXSW.   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/iPhone app that you'll likely appreciate for its design. Also, they're giving free Ts at #SXSW   \n",
       "2                                                              @swonderlin Can not wait for #iPad 2 also. They should sale them down at #SXSW.   \n",
       "3                                                           @sxsw I hope this year's festival isn't as crashy as this year's iPhone app. #sxsw   \n",
       "4          @sxtxstate great stuff on Fri #SXSW: Marissa Mayer (Google), Tim O'Reilly (tech books/conferences) &amp; Matt Mullenweg (Wordpress)   \n",
       "\n",
       "          directed_at emotion_label   brand  \\\n",
       "0              iPhone      Negative   Apple   \n",
       "1  iPad or iPhone App      Positive   Apple   \n",
       "2                iPad      Positive   Apple   \n",
       "3  iPad or iPhone App      Negative   Apple   \n",
       "4              Google      Positive  Google   \n",
       "\n",
       "                                                                                                       tweet_text_clean  \\\n",
       "0         i have a 3g iphone after  hrs tweeting at #riseaustin it was dead  i need to upgrade plugin stations at #sxsw   \n",
       "1   know about   awesome ipadiphone app that youll likely appreciate for its design also theyre giving free ts at #sxsw   \n",
       "2                                                      can not wait for #ipad  also they should sale them down at #sxsw   \n",
       "3                                              i hope this years festival isnt as crashy as this years iphone app #sxsw   \n",
       "4          great stuff on fri #sxsw marissa mayer google tim oreilly tech booksconferences amp matt mullenweg wordpress   \n",
       "\n",
       "                                                                                                                  tokens_sp  \n",
       "0                                         [g, iphone, hrs, tweet, #riseaustin, dead, need, upgrade, plugin, station, #sxsw]  \n",
       "1                                 [know, awesome, ipadiphone, app, will, likely, appreciate, design, give, free, ts, #sxsw]  \n",
       "2                                                                                                [wait, #ipad, sale, #sxsw]  \n",
       "3                                                             [hope, year, festival, not, crashy, year, iphone, app, #sxsw]  \n",
       "4  [great, stuff, fri, #sxsw, marissa, mayer, google, tim, oreilly, tech, booksconference, amp, matt, mullenweg, wordpress]  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "stops_sp = nlp.Defaults.stop_words\n",
    "print(f'spaCy Stopword Count: {len(stops_sp)}')\n",
    "\n",
    "def clean_token(doc):\n",
    "    return [token.lemma_ for token in doc if not token.is_stop \n",
    "            and not token.is_punct and not token.is_digit \n",
    "            and not token.is_space]\n",
    "\n",
    "tweets['tokens_sp'] = [clean_token(nlp(row)) for row in tweets.tweet_text_clean.apply(str)]\n",
    "\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T18:23:10.171736Z",
     "start_time": "2021-05-06T18:23:10.142375Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 9533\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('#sxsw', 8947),\n",
       " ('link', 4300),\n",
       " ('rt', 2953),\n",
       " ('ipad', 2245),\n",
       " ('google', 2102),\n",
       " ('apple', 1826),\n",
       " ('store', 1501),\n",
       " ('iphone', 1258),\n",
       " ('new', 1093),\n",
       " ('app', 992),\n",
       " ('austin', 848),\n",
       " ('launch', 819),\n",
       " ('amp', 724),\n",
       " ('social', 623),\n",
       " ('popup', 600),\n",
       " ('today', 573),\n",
       " ('open', 514),\n",
       " ('not', 496),\n",
       " ('sxsw', 481),\n",
       " ('network', 472),\n",
       " ('line', 446),\n",
       " ('circle', 444),\n",
       " ('android', 435),\n",
       " ('#apple', 409),\n",
       " ('party', 380)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "word_dict = {}\n",
    "\n",
    "# Loop through all the tags\n",
    "for i, row in tweets['tokens_sp'].iteritems():\n",
    "    for word in row:\n",
    "        if word not in word_dict:\n",
    "            word_dict[word] = 1\n",
    "        else:\n",
    "            word_dict[word] +=1\n",
    "\n",
    "word_counts = sorted(word_dict.items(), key=lambda kv: kv[1], reverse=True)\n",
    "print(f'Total words: {len(word_counts)}')\n",
    "word_counts[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T18:32:58.926604Z",
     "start_time": "2021-05-06T18:32:58.910596Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0           i have a 3g iphone after  hrs tweeting at #riseaustin it was dead  i need to upgrade plugin stations at #sxsw\n",
      "1     know about   awesome ipadiphone app that youll likely appreciate for its design also theyre giving free ts at #sxsw\n",
      "2                                                        can not wait for #ipad  also they should sale them down at #sxsw\n",
      "Name: tweet_text_clean, dtype: object    Negative  Neutral  Positive\n",
      "0         1        0         0\n",
      "1         0        0         1\n",
      "2         0        0         1\n"
     ]
    }
   ],
   "source": [
    "# filter tweets for identifiable emotions only (drop unknown)\n",
    "sentiments = ['Positive', 'Negative', 'Neutral']\n",
    "tweets_f = tweets[tweets['emotion_label'].isin(sentiments)]\n",
    "\n",
    "# create X and y (one-hot encoded for 3 classes)\n",
    "Xt = tweets_f['tokens_sp']\n",
    "X = tweets_f['tweet_text_clean']\n",
    "y = pd.get_dummies(tweets_f['emotion_label'])\n",
    "print(X.iloc[:3], y.iloc[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T18:33:30.033992Z",
     "start_time": "2021-05-06T18:33:29.960660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xt_train: (7148,) Xt_test: (1788,) yt_train: (7148, 3) yt_test: (1788, 3)\n"
     ]
    }
   ],
   "source": [
    "# Split into training and test sets for tokenized words (spacy)\n",
    "SEED = 19\n",
    "Xt_train, Xt_test, yt_train, yt_test = train_test_split(\n",
    "    Xt, y, test_size=0.20, stratify=y, random_state=SEED)\n",
    "print(f'Xt_train: {Xt_train.shape} Xt_test: {Xt_test.shape} ' \n",
    "      f'yt_train: {yt_train.shape} yt_test: {yt_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T18:33:33.803943Z",
     "start_time": "2021-05-06T18:33:33.729343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (7148,) X_test: (1788,) y_train: (7148, 3) y_test: (1788, 3)\n"
     ]
    }
   ],
   "source": [
    "# Split into training and test sets for cleaned text\n",
    "SEED = 19\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=SEED)\n",
    "print(f'X_train: {X_train.shape} X_test: {X_test.shape} ' \n",
    "      f'y_train: {y_train.shape} y_test: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T19:05:11.249568Z",
     "start_time": "2021-05-06T19:05:11.244142Z"
    }
   },
   "outputs": [],
   "source": [
    "# return max length (words) of a feature\n",
    "def FindMaxLength(lst):\n",
    "    maxList = max(lst, key = lambda i: len(i))\n",
    "    maxLength = len(maxList)    \n",
    "    return maxLength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## m1 - LSTM Embed Tweets Cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T19:57:59.338547Z",
     "start_time": "2021-05-06T19:57:59.099598Z"
    }
   },
   "outputs": [],
   "source": [
    "# keras tokenize sequences with padding\n",
    "t1 = text.Tokenizer(oov_token=1)\n",
    "t1.fit_on_texts(X_train)\n",
    "t1_tweets = t1.texts_to_sequences(X_train)\n",
    "X_train_m1 = sequence.pad_sequences(t1_tweets, padding='post') # longest 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T20:04:34.709218Z",
     "start_time": "2021-05-06T20:04:34.700055Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 7148 | Vocab size: 9557 | Input length: 30 \n"
     ]
    }
   ],
   "source": [
    "# set parameters for model input\n",
    "vocab_size_m1 = len(t1.word_index) + 1\n",
    "input_length_m1 = FindMaxLength(X_train_m1)\n",
    "\n",
    "print(f'Token count: {t1.document_count} | '\n",
    "      f'Vocab size: {vocab_size_m1} | '\n",
    "      f'Input length: {input_length_m1} '\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T20:04:44.113470Z",
     "start_time": "2021-05-06T20:04:43.932615Z"
    }
   },
   "outputs": [],
   "source": [
    "# instantiate keras sequential LSTM model layers with embeddings\n",
    "m1 = Sequential(name=\"m1_seq_lstm\")\n",
    "m1.add(Embedding(input_dim=vocab_size_m1, \n",
    "                    output_dim=128, \n",
    "                    input_length=input_length_m1\n",
    "                    ))\n",
    "m1.add(LSTM(64, return_sequences=True))\n",
    "m1.add(GlobalMaxPool1D()) # downsamples input by taking the maximum value over the time dimension\n",
    "m1.add(Dropout(0.05)) # drop out lower or remove (regularization)\n",
    "m1.add(Dense(32, activation='relu'))\n",
    "m1.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T20:04:46.631353Z",
     "start_time": "2021-05-06T20:04:46.571706Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"m1_seq_lstm\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 30, 128)           1223296   \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 30, 64)            49408     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_10 (Glo (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 1,274,883\n",
      "Trainable params: 1,274,883\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# compile model and print summary\n",
    "m1.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "m1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T20:07:23.336369Z",
     "start_time": "2021-05-06T20:04:49.160202Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cutterback/opt/anaconda3/envs/p37env/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6075 samples, validate on 1073 samples\n",
      "Epoch 1/50\n",
      "6075/6075 [==============================] - 20s 3ms/step - loss: 0.8298 - accuracy: 0.6127 - val_loss: 0.7629 - val_accuracy: 0.6486\n",
      "Epoch 2/50\n",
      "6075/6075 [==============================] - 19s 3ms/step - loss: 0.6369 - accuracy: 0.7251 - val_loss: 0.6972 - val_accuracy: 0.6859\n",
      "Epoch 3/50\n",
      "6075/6075 [==============================] - 19s 3ms/step - loss: 0.4345 - accuracy: 0.8285 - val_loss: 0.7648 - val_accuracy: 0.6710\n",
      "Epoch 4/50\n",
      "6075/6075 [==============================] - 19s 3ms/step - loss: 0.2998 - accuracy: 0.8815 - val_loss: 0.8962 - val_accuracy: 0.6775\n",
      "Epoch 5/50\n",
      "6075/6075 [==============================] - 19s 3ms/step - loss: 0.2303 - accuracy: 0.9050 - val_loss: 0.9377 - val_accuracy: 0.6626\n",
      "Epoch 6/50\n",
      "6075/6075 [==============================] - 19s 3ms/step - loss: 0.1922 - accuracy: 0.9180 - val_loss: 1.0444 - val_accuracy: 0.6608\n",
      "Epoch 7/50\n",
      "6075/6075 [==============================] - 19s 3ms/step - loss: 0.1630 - accuracy: 0.9289 - val_loss: 1.1481 - val_accuracy: 0.6580\n",
      "Epoch 8/50\n",
      "6075/6075 [==============================] - 19s 3ms/step - loss: 0.1429 - accuracy: 0.9356 - val_loss: 1.2588 - val_accuracy: 0.6636\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7ff42db6a990>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the callbacks, early stopping and save final model\n",
    "early_stop_m1 = [EarlyStopping(monitor='val_loss', patience=6), \n",
    "                  ModelCheckpoint(filepath='best_model_m1.h5', monitor='val_loss',\n",
    "                                  save_best_only=True)]\n",
    "\n",
    "m1.fit(X_train_m1, y_train, epochs=50, callbacks=early_stop_m1,\n",
    "          validation_split=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## m2 - LSTM Embed Tweet Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T19:09:17.082172Z",
     "start_time": "2021-05-06T19:09:16.953331Z"
    }
   },
   "outputs": [],
   "source": [
    "# keras tokenize sequences with padding\n",
    "t2 = text.Tokenizer(oov_token=1)\n",
    "t2.fit_on_texts(Xt_train)\n",
    "t2_tweets = t2.texts_to_sequences(Xt_train)\n",
    "Xt_train_m2 = sequence.pad_sequences(t2_tweets, padding='post') # longest 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T19:10:12.250782Z",
     "start_time": "2021-05-06T19:10:12.240516Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 7148 | Vocab size: 8497 | Input length: 23 \n"
     ]
    }
   ],
   "source": [
    "# set parameters for model input\n",
    "vocab_size = len(t2.word_index) + 1\n",
    "input_length = FindMaxLength(Xt_train)\n",
    "\n",
    "print(f'Token count: {t2.document_count} | '\n",
    "      f'Vocab size: {vocab_size} | '\n",
    "      f'Input length: {input_length} '\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T19:19:03.623552Z",
     "start_time": "2021-05-06T19:19:03.473097Z"
    }
   },
   "outputs": [],
   "source": [
    "# instantiate keras sequential LSTM model layers with embeddings\n",
    "model = Sequential(name=\"m2_seq_lstm\")\n",
    "model.add(Embedding(input_dim=vocab_size, \n",
    "                    output_dim=64, \n",
    "                    input_length=input_length\n",
    "                    ))\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(GlobalMaxPool1D()) # downsamples input by taking the maximum value over the time dimension\n",
    "model.add(Dropout(0.05)) # drop out lower or remove (regularization)\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T19:19:05.637760Z",
     "start_time": "2021-05-06T19:19:05.578880Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"m2_seq_lstm\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 23, 64)            543808    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 23, 64)            33024     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_6 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 579,011\n",
      "Trainable params: 579,011\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# compile model and print summary\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-06T19:20:58.076386Z",
     "start_time": "2021-05-06T19:19:11.875547Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cutterback/opt/anaconda3/envs/p37env/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6075 samples, validate on 1073 samples\n",
      "Epoch 1/50\n",
      "6075/6075 [==============================] - 15s 2ms/step - loss: 0.8475 - accuracy: 0.6086 - val_loss: 0.7833 - val_accuracy: 0.6384\n",
      "Epoch 2/50\n",
      "6075/6075 [==============================] - 13s 2ms/step - loss: 0.6647 - accuracy: 0.7091 - val_loss: 0.7404 - val_accuracy: 0.6785\n",
      "Epoch 3/50\n",
      "6075/6075 [==============================] - 13s 2ms/step - loss: 0.4669 - accuracy: 0.8089 - val_loss: 0.7916 - val_accuracy: 0.6766\n",
      "Epoch 4/50\n",
      "6075/6075 [==============================] - 13s 2ms/step - loss: 0.3402 - accuracy: 0.8649 - val_loss: 0.8259 - val_accuracy: 0.6785\n",
      "Epoch 5/50\n",
      "6075/6075 [==============================] - 13s 2ms/step - loss: 0.2624 - accuracy: 0.8895 - val_loss: 0.9877 - val_accuracy: 0.6822\n",
      "Epoch 6/50\n",
      "6075/6075 [==============================] - 13s 2ms/step - loss: 0.2147 - accuracy: 0.9108 - val_loss: 0.9733 - val_accuracy: 0.6673\n",
      "Epoch 7/50\n",
      "6075/6075 [==============================] - 13s 2ms/step - loss: 0.1828 - accuracy: 0.9198 - val_loss: 1.1884 - val_accuracy: 0.6785\n",
      "Epoch 8/50\n",
      "6075/6075 [==============================] - 13s 2ms/step - loss: 0.1643 - accuracy: 0.9254 - val_loss: 1.2960 - val_accuracy: 0.6645\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7ff42cd140d0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the callbacks, early stopping and save final model\n",
    "early_stop = [EarlyStopping(monitor='val_loss', patience=6), \n",
    "                  ModelCheckpoint(filepath='best_model_m2.h5', monitor='val_loss',\n",
    "                                  save_best_only=True)]\n",
    "\n",
    "model.fit(Xt_train_m2, yt_train, epochs=50, callbacks=early_stop,\n",
    "          validation_split=0.15) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes \n",
    "- Visualize n-grams or word clouts or frequency association\n",
    "- Word embeddings - Glove vectors\n",
    "- Encode with Bag of Words/TGIDF with RF (helps with interpretability) - helps link complexity; still could use NN (basic dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## m3 - LSTM Embed Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T13:58:35.646058Z",
     "start_time": "2021-05-07T13:58:35.476468Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tweet vocabulary is: 10563\n"
     ]
    }
   ],
   "source": [
    "# establish total vocabulary prior to fetching Glove vectors\n",
    "t0 = text.Tokenizer(oov_token=1)\n",
    "t0.fit_on_texts(X)\n",
    "vocab = list(t0.word_counts.keys())\n",
    "print(f'Total tweet vocabulary is: {len(vocab)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T14:01:17.293334Z",
     "start_time": "2021-05-07T13:58:42.752014Z"
    }
   },
   "outputs": [],
   "source": [
    "# create glove dictionary of embedding vectors for vocab list\n",
    "glove_file = 'data/glove.twitter.27B.200d.txt'\n",
    "glove = {}\n",
    "with open(glove_file, 'rb') as f:\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode('utf-8')\n",
    "        if word in vocab:\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            glove[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T14:31:14.397699Z",
     "start_time": "2021-05-07T14:31:14.160164Z"
    }
   },
   "outputs": [],
   "source": [
    "# establish total vocabulary prior to fetching Glove vectors\n",
    "t3 = text.Tokenizer(oov_token=1)\n",
    "t3.fit_on_texts(X_train)\n",
    "t3_tweets = t3.texts_to_sequences(X_train)\n",
    "X_train_m3 = sequence.pad_sequences(t3_tweets, padding='post') # longest 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T14:31:17.644382Z",
     "start_time": "2021-05-07T14:31:17.613069Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet count: 7148 | Vocab size: 9557 | Input length: 30 \n"
     ]
    }
   ],
   "source": [
    "# \n",
    "vocab_size = len(t3.word_index) + 1\n",
    "glove_dim = len(next(iter(glove.values())))  # Number of dimensions of the GloVe word embeddings\n",
    "input_length = FindMaxLength(X_train_m3)\n",
    "\n",
    "emb_matrix = np.zeros((vocab_size, glove_dim))\n",
    "for w, i in t3.word_index.items():\n",
    "    vect = glove.get(w)\n",
    "    if vect is not None:\n",
    "        emb_matrix[i] = vect\n",
    "\n",
    "print(f'Tweet count: {t3.document_count} | '\n",
    "      f'Vocab size: {vocab_size} | '\n",
    "      f'Input length: {input_length} '\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T14:34:12.835507Z",
     "start_time": "2021-05-07T14:34:12.461666Z"
    }
   },
   "outputs": [],
   "source": [
    "# instantiate keras sequential LSTM model layers with embeddings\n",
    "m3 = Sequential(name=\"m3_seq_lstm_glove\")\n",
    "m3.add(Embedding(input_dim=vocab_size, \n",
    "                    output_dim=glove_dim, \n",
    "                    input_length=input_length\n",
    "                    ))\n",
    "m3.add(LSTM(64, return_sequences=True))\n",
    "m3.add(GlobalMaxPool1D()) # downsamples input by taking the maximum value over the time dimension\n",
    "m3.add(Dropout(0.05)) # drop out lower or remove (regularization)\n",
    "m3.add(Dense(32, activation='relu'))\n",
    "m3.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T14:34:15.031628Z",
     "start_time": "2021-05-07T14:34:15.013868Z"
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "m3.layers[0].set_weights([emb_matrix])\n",
    "m3.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T14:34:18.506624Z",
     "start_time": "2021-05-07T14:34:18.431398Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"m3_seq_lstm_glove\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 30, 200)           1911400   \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 30, 64)            67840     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_11 (Glo (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 3)                 99        \n",
      "=================================================================\n",
      "Total params: 1,981,419\n",
      "Trainable params: 70,019\n",
      "Non-trainable params: 1,911,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# compile model and print summary\n",
    "m3.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "m3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T14:37:25.905197Z",
     "start_time": "2021-05-07T14:34:32.066251Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6075 samples, validate on 1073 samples\n",
      "Epoch 1/50\n",
      "6075/6075 [==============================] - 17s 3ms/step - loss: 0.8149 - accuracy: 0.6176 - val_loss: 0.7471 - val_accuracy: 0.6542\n",
      "Epoch 2/50\n",
      "6075/6075 [==============================] - 15s 3ms/step - loss: 0.7103 - accuracy: 0.6747 - val_loss: 0.6847 - val_accuracy: 0.6952\n",
      "Epoch 3/50\n",
      "6075/6075 [==============================] - 15s 3ms/step - loss: 0.6402 - accuracy: 0.7093 - val_loss: 0.6477 - val_accuracy: 0.6980\n",
      "Epoch 4/50\n",
      "6075/6075 [==============================] - 15s 3ms/step - loss: 0.5853 - accuracy: 0.7407 - val_loss: 0.6439 - val_accuracy: 0.7167\n",
      "Epoch 5/50\n",
      "6075/6075 [==============================] - 15s 3ms/step - loss: 0.5317 - accuracy: 0.7656 - val_loss: 0.6341 - val_accuracy: 0.7046\n",
      "Epoch 6/50\n",
      "6075/6075 [==============================] - 15s 3ms/step - loss: 0.4888 - accuracy: 0.7918 - val_loss: 0.6429 - val_accuracy: 0.6999\n",
      "Epoch 7/50\n",
      "6075/6075 [==============================] - 16s 3ms/step - loss: 0.4329 - accuracy: 0.8206 - val_loss: 0.6753 - val_accuracy: 0.7064\n",
      "Epoch 8/50\n",
      "6075/6075 [==============================] - 16s 3ms/step - loss: 0.3897 - accuracy: 0.8382 - val_loss: 0.6928 - val_accuracy: 0.6878\n",
      "Epoch 9/50\n",
      "6075/6075 [==============================] - 17s 3ms/step - loss: 0.3535 - accuracy: 0.8540 - val_loss: 0.7062 - val_accuracy: 0.6822\n",
      "Epoch 10/50\n",
      "6075/6075 [==============================] - 16s 3ms/step - loss: 0.3131 - accuracy: 0.8775 - val_loss: 0.7592 - val_accuracy: 0.6990\n",
      "Epoch 11/50\n",
      "6075/6075 [==============================] - 16s 3ms/step - loss: 0.2769 - accuracy: 0.8937 - val_loss: 0.7684 - val_accuracy: 0.6859\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7ff3bc65fa90>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the callbacks, early stopping and save final model\n",
    "early_stop_m3 = [EarlyStopping(monitor='val_loss', patience=6), \n",
    "                  ModelCheckpoint(filepath='best_model_m3.h5', monitor='val_loss',\n",
    "                                  save_best_only=True)]\n",
    "\n",
    "m3.fit(X_train_m3, y_train, epochs=50, callbacks=early_stop_m3,\n",
    "          validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:p37env] *",
   "language": "python",
   "name": "conda-env-p37env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "358.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
